library(dplyr)
library(tidyr)
set.seed(12604)

# This part of the lab will illustrate a problem with over-fitting
# models: over-fit models tend to perform worse on novel data.

# To illustrate, let's take a look at a simple statistical model.

# I'm going to start by defining a function that takes an
# observation, x, and generates data y.
# The data follow a cubic polynomial function (i.e., a curvy line.)

data.generating.process <- function(x) {
  b.0 <- 0
  b.1 <- 7
  b.2 <- -2
  b.3 <- 0.13
  y <- b.0 + b.1*x + b.2*x^2 + b.3*x^3
  return(y)
}

# Now I'll create a set of values, x, and use the
# data generating process with some random noise
# to create a simulated data set.

simulated.data <- data.frame(x = seq(0,12,0.5)) %>%
  mutate(y = data.generating.process(x) + rnorm(n(), 0, 3))

# Let's generate a plot of the data.

plot(y ~ x, data=simulated.data)

# We can add the true data generating process (minus noise)
# to this plot to visualize the how the noise affected
# the data

simulated.data <- simulated.data %>%
  mutate(y.true = data.generating.process(x))

points(y.true ~ x, data=simulated.data, col="red", type="l")

# This fit looks pretty good, but that's because we're generating
# the data from this exact model! 

## PREDICTION FUNCTIONS ########

# Let's start by generalizing what I've done above to a few different models.
# We want to be able to pass in a set of parameters and some data, and get
# the model's predictions back.

# Create prediction functions for the 1-degree model, 3-degree model, 
# and 7-degree model.

model.1.prediction <- function(params, x){
  return(params[1] + params[2]*x)
}

model.3.prediction <- function(params, x){
  return(params[1] + params[2]*x + params[3]*x^2 + params[4]*x^3)
}

model.7.prediction <- function(params, x){
  return(params[1] + params[3]*x + params[3]*x^2 + params[4]*x^3 + params[5]*x^4 + params[6]*x^5 + params[7]*x^6 + params[8]*x^7)
}


## LIKELIHOOD FUNCTIONS #######

# We need to define the likelihood for these models. We'll imagine that the
# data are generated by sampling from a normal distribution with the mean 
# centered at the line predicted by the polynomial function. 

model.1.discrepancy <- function(params, data){
  log.likelihood <- data %>%
    mutate(y.pred = model.1.prediction(params, x)) %>%
    mutate(log.p = dnorm(y, mean=y.pred, sd=3, log = TRUE)) %>%
    pull(log.p) %>%
    sum()
  
  return(-log.likelihood)
}

model.3.discrepancy <- function(params, data){
  log.likelihood <- data %>%
    mutate(y.pred = model.3.prediction(params, x)) %>%
    mutate(log.p = dnorm(y, mean=y.pred, sd=3, log = TRUE)) %>%
    pull(log.p) %>%
    sum()
  
  return(-log.likelihood)
}

model.7.discrepancy <- function(params, data){
  log.likelihood <- data %>%
    mutate(y.pred = model.7.prediction(params, x)) %>%
    mutate(log.p = dnorm(y, mean=y.pred, sd=3, log = TRUE)) %>%
    pull(log.p) %>%
    sum()
  
  return(-log.likelihood)
}


## FITTING THE MODELS ######

# first, just plot the data

plot(y ~ x, data = simulated.data)

# fill in the code to run the MLE

model.1.result <- optim(model.1.discrepancy, par=c(0,0), data=simulated.data)

simulated.data <- simulated.data %>%
  mutate(y.pred.1 = model.1.prediction(model.1.result$par, x))

points(y.pred.1 ~ x, data=simulated.data, type="l", col="purple")



model.3.result <- optim(model.3.discrepancy, par=c(0,0,0,0), data=simulated.data, control = list(maxit=2000))

simulated.data <- simulated.data %>%
  mutate(y.pred.3 = model.3.prediction(model.3.result$par, x))

points(y.pred.3 ~ x, data=simulated.data, type="l", col="blue")


model.7.result <- optim(model.7.discrepancy, par=c(0,0,0,0,0,0,0,0), data=simulated.data, control = list(maxit=2000))

simulated.data <- simulated.data %>%
  mutate(y.pred.7 = model.7.prediction(model.7.result$par, x))

points(y.pred.7 ~ x, data=simulated.data, type="l", col="green")


## LIKELIHOOD RATIO TEST ####

# What's the deviance (-2lnL) for model 1 and model 3?

model.1.deviance <- 2*model.1.result$value
model.3.deviance <- 2*model.3.result$value

# formula for LR test statistic is -2lnL(specific) - -2lnL(general)
# specific = fewer parameters
# general = more parameters

test.statistic <- model.1.deviance - model.3.deviance

# then we estimate how unlikely it would be to observe a value equal
# to or greater than this test statistic in a chi-sq distribution with
# K degrees of freedom, where K = number of additional parameters
# in the general model.

K <- 2
1 - pchisq(test.statistic, K)

# What's the deviance (-2lnL) for model 3 and model 7?

model.7.deviance <- 2*model.7.result$value

# formula for LR test statistic is -2lnL(specific) - -2lnL(general)
# specific = fewer parameters
# general = more parameters

test.statistic <- model.3.deviance - model.7.deviance

# then we estimate how unlikely it would be to observe a value equal
# to or greater than this test statistic in a chi-sq distribution with
# K degrees of freedom, where K = number of additional parameters
# in the general model.

K <- 4
1 - pchisq(test.statistic, K)

## AIC / BIC #######

# AIC and BIC are information criteria -- heuristics for comparing
# models based on weighting the fit vs. the complexity of the models

# AIC is derived from an information theoretic perspective
# BIC is derived from a Bayesian perspective. 

# They are both commonly used as metrics.

# AIC 

# To calculate the AIC of a model, we use
# AIC <- -2ln(L) + 2K
# where K is the number of parameters in the model

# Calculate the AIC for all three models here:

model.1.AIC <- model.1.deviance + 2*2
model.3.AIC <- model.3.deviance + 2*4
model.7.AIC <- model.7.deviance + 2*8


# For AIC, a smaller AIC is preferable to a large AIC. 
# Differences in AIC of >10 are thought to be relatively decisive.
# Which model is preferred here?

model.1.AIC
model.3.AIC
model.7.AIC


# BIC

# To calculate the BIC of a model, we use
# BIC <- k*ln(N) - 2ln(L)
# where K is number of parameters in the model
# and N is the number of samples being fit by the model

model.1.BIC <- model.1.deviance + 2*log(nrow(Rsimulated.data))
model.3.BIC <- model.3.deviance + 4*log(nrow(simulated.data))
model.7.BIC <- model.7.deviance + 8*log(nrow(simulated.data))

model.1.BIC
model.3.BIC
model.7.BIC

# For BIC we also prefer smaller values, and difference >10 are thought
# to be relatively decisive as well.
# Which model is preferred here?




