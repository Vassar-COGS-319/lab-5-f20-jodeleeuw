library(dplyr)
library(tidyr)
set.seed(12604)

# This part of the lab will illustrate a problem with over-fitting
# models: over-fit models tend to perform worse on novel data.

# To illustrate, let's take a look at a simple statistical model.

# I'm going to start by defining a function that takes an
# observation, x, and generates data y.
# The data follow a cubic polynomial function (i.e., a curvy line.)

data.generating.process <- function(x) {
  b.0 <- 0
  b.1 <- 7
  b.2 <- -2
  b.3 <- 0.13
  y <- b.0 + b.1*x + b.2*x^2 + b.3*x^3
  return(y)
}

# Now I'll create a set of values, x, and use the
# data generating process with some random noise
# to create a simulated data set.

simulated.data <- data.frame(x = seq(0,12,0.5)) %>%
  mutate(y = data.generating.process(x) + rnorm(n(), 0, 3))

# Let's generate a plot of the data.

plot(y ~ x, data=simulated.data)

# We can add the true data generating process (minus noise)
# to this plot to visualize the how the noise affected
# the data

simulated.data <- simulated.data %>%
  mutate(y.true = data.generating.process(x))

points(y.true ~ x, data=data, col="red", type="l")

# This fit looks pretty good, but that's because we're generating
# the data from this exact model! 

## PREDICTION FUNCTIONS ########

# Let's start by generalizing what I've done above to a few different models.
# We want to be able to pass in a set of parameters and some data, and get
# the model's predictions back.

# Create prediction functions for the 1-degree model, 3-degree model, 
# and 7-degree model.

model.1.prediction <- function(params, x){
  
}

model.3.prediction <- function(params, x){
  
}

model.7.prediction <- function(params, x){

}


## LIKELIHOOD FUNCTIONS #######

# We need to define the likelihood for these models. We'll imagine that the
# data are generated by sampling from a normal distribution with the mean 
# centered at the line predicted by the polynomial function. 

model.1.discrepancy <- function(params, data){
  
  
}

model.3.discrepancy <- function(params, data){
  
  
}

model.7.discrepancy <- function(params, data){
  
  
}


## FITTING THE MODELS ######

# first, just plot the data

plot(y ~ x, data = simulated.data)

# fill in the code to run the MLE

model.1.result <- NA

simulated.data <- simulated.data %>%
  mutate(y.pred.1 = model.1.prediction(model.1.result$par, x))

points(y.pred.1 ~ x, data=simulated.data, type="l", col="purple")



model.3.result <- NA

simulated.data <- simulated.data %>%
  mutate(y.pred.3 = model.3.prediction(model.3.result$par, x))

points(y.pred.3 ~ x, data=simulated.data, type="l", col="blue")


model.7.result <- NA

simulated.data <- simulated.data %>%
  mutate(y.pred.7 = model.7.prediction(model.7.result$par, x))

points(y.pred.7 ~ x, data=simulated.data, type="l", col="green")


## LIKELIHOOD RATIO TEST ####

# What's the deviance (-2lnL) for model 1 and model 3?



# formula for LR test statistic is -2lnL(specific) - -2lnL(general)
# specific = fewer parameters
# general = more parameters

test.statistic <- NA

# then we estimate how unlikely it would be to observe a value equal
# to or greater than this test statistic in a chi-sq distribution with
# K degrees of freedom, where K = number of additional parameters
# in the general model.

K <- NA
1 - pchisq(test.statistic, K)

# What's the deviance (-2lnL) for model 3 and model 7?


# formula for LR test statistic is -2lnL(specific) - -2lnL(general)
# specific = fewer parameters
# general = more parameters

test.statistic <- NA

# then we estimate how unlikely it would be to observe a value equal
# to or greater than this test statistic in a chi-sq distribution with
# K degrees of freedom, where K = number of additional parameters
# in the general model.

K <- NA
1 - pchisq(test.statistic, K)

## AIC / BIC #######

# AIC and BIC are information criteria -- heuristics for comparing
# models based on weighting the fit vs. the complexity of the models

# AIC is derived from an information theoretic perspective
# BIC is derived from a Bayesian perspective. 

# They are both commonly used as metrics.

# AIC 

# To calculate the AIC of a model, we use
# AIC <- -2ln(L) + 2K
# where K is the number of parameters in the model

# Calculate the AIC for all three models here:


# For AIC, a smaller AIC is preferable to a large AIC. 
# Differences in AIC of >10 are thought to be relatively decisive.
# Which model is preferred here?


# BIC

# To calculate the BIC of a model, we use
# BIC <- k*ln(N) - 2ln(L)
# where K is number of parameters in the model
# and N is the number of samples being fit by the model

# For BIC we also prefer smaller values, and difference >10 are thought
# to be relatively decisive as well.
# Which model is preferred here?




